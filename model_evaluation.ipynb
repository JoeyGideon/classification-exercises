{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the following confusion matrix, evaluate (by hand) the model's performance.\n",
    "\n",
    "\n",
    "|               | pred dog   | pred cat   |\n",
    "|:------------  |-----------:|-----------:|\n",
    "| actual dog    |         46 |         7  |\n",
    "| actual cat    |         13 |         34 |\n",
    "In the context of this problem, what is a false positive?\n",
    "In the context of this problem, what is a false negative?\n",
    "How would you describe this model?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Accuracy:** Overall, how often is the classifier correct?\n",
    "\n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "where TP = True positive, TN = True negative, FP = False positive, FN = False negative.\n",
    "\n",
    "In this case, TP = 46, TN = 34, FP = 13, and FN = 7.\n",
    "\n",
    "Accuracy = (46 + 34) / (46 + 34 + 13 + 7) = 0.8, or 80%.\n",
    "\n",
    "The overall accuracy of the model is 80%.\n",
    "\n",
    "**Precision:** When the model predicts \"dog\", how often is it correct?\n",
    "\n",
    "Precision = TP / (TP + FP)\n",
    "\n",
    "In this case, TP = 46 and FP = 13.\n",
    "\n",
    "Precision = 46 / (46 + 13) = 0.78, or 78%.\n",
    "\n",
    "The precision of the model for \"dog\" is 78%.\n",
    "\n",
    "**Recall:** When the actual value is \"dog\", how often does the model predict \"dog\"?\n",
    "\n",
    "Recall = TP / (TP + FN)\n",
    "\n",
    "In this case, TP = 46 and FN = 7.\n",
    "\n",
    "Recall = 46 / (46 + 7) = 0.87, or 87%.\n",
    "\n",
    "The recall of the model for \"dog\" is 87%.\n",
    "\n",
    "**F1 score:** The harmonic mean of precision and recall.\n",
    "\n",
    "F1 score = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "\n",
    "F1 score = 2 * (0.78 * 0.87) / (0.78 + 0.87) = 0.82, or 82%.\n",
    "\n",
    "The F1 score of the model is 82%.\n",
    "\n",
    "Overall, the model has a relatively good performance with an accuracy of 80%, precision of 78%, recall of 87%, and F1 score of 82%.\n",
    "\n",
    "### In the context of this problem, a false positive occurs when the model predicts that an image is of a dog, but the actual image is of a cat. In other words, the model has incorrectly identified an image as a dog when it is actually a cat. A false positive is represented in the confusion matrix by the number of actual cat images that were incorrectly classified as dog images (13 in this case).\n",
    "\n",
    "### In the context of this problem, a false negative occurs when the model predicts that an image is of a cat, but the actual image is of a dog. In other words, the model has incorrectly identified an image as a cat when it is actually a dog. A false negative is represented in the confusion matrix by the number of actual dog images that were incorrectly classified as cat images (7 in this case).\n",
    "\n",
    "### Based on the confusion matrix analysis, the model appears to be performing reasonably well, with an accuracy of 80%, precision of 78%, recall of 87%, and F1 score of 82%. However, without additional context such as the dataset size, distribution of the classes, and the model's architecture and training process, it is difficult to make a definitive statement about the model's quality.\n",
    "\n",
    "### In general, a good model should have a high accuracy, precision, recall, and F1 score, and should be able to generalize well to new, unseen data. Additionally, the model should be designed and trained with appropriate techniques such as regularization, cross-validation, and hyperparameter tuning to avoid overfitting and achieve optimal performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are working as a datascientist working for Codeup Cody Creator (C3 for short), a rubber-duck manufacturing plant.\n",
    "\n",
    "Unfortunately, some of the rubber ducks that are produced will have defects. Your team has built several models that try to predict those defects, and the data from their predictions can be found here.\n",
    "\n",
    "Use the predictions dataset and pandas to help answer the following questions:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An internal team wants to investigate the cause of the manufacturing defects. \n",
    "They tell you that they want to identify as many of the ducks that have a defect as possible. \n",
    "Which evaluation metric would be appropriate here? Which model would be the best fit for this use case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall for Model 1: 0.99\n",
      "Recall for Model 2: 0.56\n",
      "Recall for Model 3: 0.53\n"
     ]
    }
   ],
   "source": [
    "# In this case, the internal team wants to identify as many ducks with defects as possible, \n",
    "# so the evaluation metric that would be appropriate here is recall.\n",
    "#  Recall is the proportion of actual positives that are correctly identified by the model. \n",
    "\n",
    "# To determine which model is the best fit for this use case, we can calculate the recall for each model. \n",
    "# We will need to know the true positives (TP), false positives (FP), true negatives (TN), and false negatives (FN)\n",
    "# for each model to calculate recall.\n",
    "\n",
    "# We can calculate these values using the following code:\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "# Load the predictions dataset\n",
    "predictions = pd.read_csv('c3.csv')\n",
    "\n",
    "\n",
    "# Calculate confusion matrix for each model\n",
    "model1_cm = confusion_matrix(predictions.actual, predictions.model1)\n",
    "model2_cm = confusion_matrix(predictions.actual, predictions.model2)\n",
    "model3_cm = confusion_matrix(predictions.actual, predictions.model3)\n",
    "\n",
    "# Calculate classification report for each model\n",
    "model1_cr = classification_report(predictions.actual, predictions.model1)\n",
    "model2_cr = classification_report(predictions.actual, predictions.model2)\n",
    "model3_cr = classification_report(predictions.actual, predictions.model3)\n",
    "\n",
    "# Calculate recall for each model\n",
    "model1_recall = model1_cm[1, 1] / (model1_cm[1, 1] + model1_cm[1, 0])\n",
    "model2_recall = model2_cm[1, 1] / (model2_cm[1, 1] + model2_cm[1, 0])\n",
    "model3_recall = model3_cm[1, 1] / (model3_cm[1, 1] + model3_cm[1, 0])\n",
    "\n",
    "# Print the recall for each model\n",
    "print(\"Recall for Model 1: {:.2f}\".format(model1_recall))\n",
    "print(\"Recall for Model 2: {:.2f}\".format(model2_recall))\n",
    "print(\"Recall for Model 3: {:.2f}\".format(model3_recall))\n",
    "\n",
    "# Based on these results, Model 1 has the highest recall of 0.99,\n",
    "# followed by Model 2 with a recall of 0.56, and Model 3 with a recall of 0.53. \n",
    "# This suggests that Model 1 is the best fit for this use case as it has the highest recall,\n",
    "# indicating it is better at identifying true positives. However, it is also important to consider other factors such as precision,\n",
    "# accuracy, and the overall business objectives before selecting the final model.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recently several stories in the local news have come out highlighting customers who received a rubber duck with a defect, and portraying C3 in a bad light. The PR team has decided to launch a program that gives customers with a defective duck a vacation to Hawaii. They need you to predict which ducks will have defects, but tell you the really don't want to accidentally give out a vacation package when the duck really doesn't have a defect. Which evaluation metric would be appropriate here? Which model would be the best fit for this use case?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation metric that would be appropriate here is precision, as the PR team wants to minimize false positives (giving a vacation package to a customer who received a rubber duck without a defect). The model that would be the best fit for this use case would be a binary classification model such as logistic regression or a decision tree, which can predict whether a duck has a defect or not based on the values in the three model columns. The PR team can then use the model's predictions to decide which customers to offer the vacation package to. To optimize for precision, the model can be trained and evaluated using precision as the evaluation metric, and the threshold for classifying a duck as defective can be adjusted to maximize precision."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are working as a data scientist for Gives You Paws â„¢, a subscription based service that shows you cute pictures of dogs or cats (or both for an additional fee)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of accuracy, how do the various models compare to the baseline model? Are any of the models better than the baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 0.6508\n",
      "Model 1 accuracy: 0.8074\n",
      "Model 2 accuracy: 0.6304\n",
      "Model 3 accuracy: 0.5096\n",
      "Model 4 accuracy: 0.7426\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('gives_you_paws.csv')\n",
    "\n",
    "# Calculate the frequency of each class\n",
    "class_freq = data['actual'].value_counts()\n",
    "\n",
    "# Get the most common class\n",
    "most_common_class = class_freq.index[0]\n",
    "\n",
    "# Calculate the accuracy of the baseline model\n",
    "total_predictions = len(data)\n",
    "correct_predictions = len(data[data['actual'] == most_common_class])\n",
    "baseline_accuracy = correct_predictions / total_predictions\n",
    "\n",
    "print('Baseline accuracy:', baseline_accuracy)\n",
    "\n",
    "# Calculate the accuracy of each model\n",
    "for i in range(1, 5):\n",
    "    model_col = 'model' + str(i)\n",
    "    correct_predictions = len(data[data[model_col] == data['actual']])\n",
    "    model_accuracy = correct_predictions / total_predictions\n",
    "    print('Model', i, 'accuracy:', model_accuracy)\n",
    "\n",
    "\n",
    "# Model 1 and Model 4 are better than the baseline model while Model 2 and Model 3 are worse."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you are working on a team that solely deals with dog pictures. Which of these models would you recommend?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would recommend using Model 1 or Model 4 for the team that solely deals with dog pictures. Both of these models have an accuracy higher than the baseline model and have a high accuracy for predicting dogs specifically.\n",
    "\n",
    "Model 1 has an accuracy of 80.74% and correctly predicted dogs 93.75% of the time, while Model 4 has an accuracy of 74.26% and correctly predicted dogs 87.5% of the time. Therefore, both of these models have a high accuracy for predicting dogs and would be suitable for a team that solely deals with dog pictures."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you are working on a team that solely deals with cat pictures. Which of these models would you recommend?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the given dataset, I would recommend using Model 4 for the team that solely deals with cat pictures. Although Model 1 has a higher overall accuracy, it correctly predicted cats only 62.5% of the time whereas Model 4 has an overall accuracy of 74.26% and correctly predicted cats 75% of the time.\n",
    "\n",
    "Therefore, Model 4 would be a better choice for a team that solely deals with cat pictures as it has a higher accuracy for predicting cats specifically."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the links below to read the documentation about each function, then apply those functions to the data from the previous problem.\n",
    "\n",
    "sklearn.metrics.accuracy_score\n",
    "sklearn.metrics.precision_score\n",
    "sklearn.metrics.recall_score\n",
    "sklearn.metrics.classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 Accuracy: 0.8074\n",
      "Model 1 Recall: 0.8033\n",
      "Model 1 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cat       0.69      0.82      0.75      1746\n",
      "         dog       0.89      0.80      0.84      3254\n",
      "\n",
      "    accuracy                           0.81      5000\n",
      "   macro avg       0.79      0.81      0.80      5000\n",
      "weighted avg       0.82      0.81      0.81      5000\n",
      "\n",
      "\n",
      "Model 2 Accuracy: 0.6304\n",
      "Model 2 Recall: 0.4908\n",
      "Model 2 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cat       0.48      0.89      0.63      1746\n",
      "         dog       0.89      0.49      0.63      3254\n",
      "\n",
      "    accuracy                           0.63      5000\n",
      "   macro avg       0.69      0.69      0.63      5000\n",
      "weighted avg       0.75      0.63      0.63      5000\n",
      "\n",
      "\n",
      "Model 3 Accuracy: 0.5096\n",
      "Model 3 Recall: 0.5086\n",
      "Model 3 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cat       0.36      0.51      0.42      1746\n",
      "         dog       0.66      0.51      0.57      3254\n",
      "\n",
      "    accuracy                           0.51      5000\n",
      "   macro avg       0.51      0.51      0.50      5000\n",
      "weighted avg       0.55      0.51      0.52      5000\n",
      "\n",
      "\n",
      "Model 4 Accuracy: 0.7426\n",
      "Model 4 Recall: 0.9557\n",
      "Model 4 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cat       0.81      0.35      0.48      1746\n",
      "         dog       0.73      0.96      0.83      3254\n",
      "\n",
      "    accuracy                           0.74      5000\n",
      "   macro avg       0.77      0.65      0.66      5000\n",
      "weighted avg       0.76      0.74      0.71      5000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('gives_you_paws.csv')\n",
    "\n",
    "# Define the target and prediction columns\n",
    "target_col = 'actual'\n",
    "prediction_cols = ['model1', 'model2', 'model3', 'model4']\n",
    "\n",
    "# Calculate and print the accuracy of each model\n",
    "\n",
    "for i, col in enumerate(prediction_cols):\n",
    "    y_true = data[target_col]\n",
    "    y_pred = data[col]\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f\"Model {i+1} Accuracy: {accuracy:.4f}\") \n",
    "\n",
    "    \n",
    "    recall = recall_score(y_true, y_pred, pos_label='dog')\n",
    "    print(f\"Model {i+1} Recall: {recall:.4f}\")\n",
    "    \n",
    "    class_report = classification_report(y_true, y_pred)\n",
    "    print(f\"Model {i+1} Classification Report:\\n{class_report}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
